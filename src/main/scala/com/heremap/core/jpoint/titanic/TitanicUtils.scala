package com.heremap.core.jpoint.titanic

import org.apache.spark.ml.Transformer
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}

object TitanicUtils {
    def getSparkSession: SparkSession = {
        //For windows only: don't forget to put winutils.exe to c:/bin folder
        System.setProperty("hadoop.home.dir", "c:\\")

        val spark = SparkSession.builder
            .master("local")
            .appName("Spark_SQL")
            .getOrCreate()

        spark.sparkContext.setLogLevel("ERROR")
        spark
    }

    def readPassengers(spark: SparkSession): DataFrame = {
        val passengers = spark.read
            .option("delimiter", ";")
            .option("inferSchema", "true")
            .option("header", "true")
            .csv("/home/zaleslaw/data/titanic.csv")

        passengers.printSchema()

        passengers.show()

        passengers
    }

    def readPassengersWithCastingToDoubles(spark: SparkSession): DataFrame = {
        val passengers = spark.read
            .option("delimiter", ";")
            .option("inferSchema", "true")
            .option("header", "true")
            .csv("/home/zaleslaw/data/titanic.csv")

        import org.apache.spark.sql
        import spark.implicits._

        val castedPassengers = passengers
            .withColumn("survived", $"survived".cast(sql.types.DoubleType))
            .withColumn("pclass", $"pclass".cast(sql.types.DoubleType))
            .withColumn("sibsp", $"sibsp".cast(sql.types.DoubleType))
            .withColumn("parch", $"parch".cast(sql.types.DoubleType))

        castedPassengers.printSchema()

        castedPassengers.show()

        castedPassengers
    }


    def readPassengersWithCasting(spark: SparkSession): DataFrame = {
        val passengers = spark.read
            .option("delimiter", ";")
            .option("inferSchema", "true")
            .option("header", "true")
            .csv("/home/zaleslaw/data/titanic.csv")

        import org.apache.spark.sql
        import spark.implicits._

        val castedPassengers = passengers
            .withColumn("survived", $"survived".cast(sql.types.DoubleType))
            .withColumn("pclass", $"pclass".cast(sql.types.DoubleType))
            .withColumn("sibsp", $"sibsp".cast(sql.types.DoubleType))
            .withColumn("parch", $"parch".cast(sql.types.DoubleType))
            .withColumn("age", $"age".cast(sql.types.DoubleType))
            .withColumn("fare", $"fare".cast(sql.types.DoubleType))

        castedPassengers.printSchema()

        castedPassengers.show()

        castedPassengers
    }

    class DropSex extends Transformer {
        private val serialVersionUID = 5545470640951989469L

        override def transform(
            dataset: Dataset[_]): DataFrame = {
            val result = dataset.drop("sex", "embarked") // <============== drop columns to use Imputer
            println("DropSex is working")
            result.show()
            result.printSchema()
            result
        }

        override def copy(
            extra: ParamMap): Transformer = null

        override def transformSchema(
            schema: StructType): StructType = schema

        override val uid: String = "CustomTransformer" + serialVersionUID
    }

    class Printer extends Transformer {
        private val serialVersionUID = 3345470640951989469L

        override def transform(
            dataset: Dataset[_]): DataFrame = {
            println(">>>>>>>>>> Printer output")
            dataset.show(false)
            dataset.printSchema()
            dataset.toDF()
        }

        override def copy(
            extra: ParamMap): Transformer = null

        override def transformSchema(
            schema: StructType): StructType = schema

        override val uid: String = "CustomTransformer" + serialVersionUID
    }
}
